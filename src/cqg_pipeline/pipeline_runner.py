"""\nMain pipeline runner coordinating all CQG components.\n\nOrchestrates the full workflow:\n1. Load transcripts\n2. Action-Driven Semantic Clustering (Cluster & Characterize)\n3. Generate diverse causal queries\n4. Export results\n"""\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom .transcript_loader import TranscriptLoader\nfrom .action_extractor import ActionDrivenExtractor\nfrom .cluster_query_generator import ClusterQueryGenerator\nfrom .llm_client import LLMClient\nfrom .config import Config\nfrom .task2_recognition import RecognitionModule\nfrom .task2_selection_llm import SelectionModuleLLM\nfrom .task2_fusion_llm import FusionModuleLLM\nfrom .task2_generation_llm import GenerationModuleLLM\nfrom .task2_utils_simple import load_qa_pairs, save_followups_csv, link_qas_to_clusters\n\nlogger = logging.getLogger(__name__)\n\n\nclass CQGPipeline:\n    """\n    Causal Query Generation Pipeline.\n    \n    Coordinates all components to generate high-quality causal queries\n    from contact center transcripts using Action-Driven Semantic Clustering.\n    """\n    \n    def __init__(\n        self,\n        llm_model: Optional[str] = None,\n    ):\n        """\n        Initialize pipeline with all components.\n        \n        Args:\n            llm_model: Model for query generation (default from config)\n        """\n        logger.info("=" * 80)\n        logger.info("Initializing Causal Query Generation Pipeline (Action-Driven)")\n        logger.info("=" * 80)\n        \n        # Initialize components\n        self.loader = TranscriptLoader()\n        \n        # Create LLM client\n        llm_model = llm_model or Config.LLM_MODEL\n        self.llm_client = LLMClient(model=llm_model)\n        \n        # New components\n        self.extractor = ActionDrivenExtractor()\n        self.generator = ClusterQueryGenerator(llm_client=self.llm_client)\n        \n        # Results storage\n        self.queries: List[str] = []\n        self.cluster_chars: Dict = {}\n        \n        # Statistics\n        self.stats = {\n            "ingested": 0,\n            "clusters_found": 0,\n            "queries_generated": 0,\n            "start_time": None,\n            "end_time": None,\n            "duration_seconds": 0\n        }\n        \n        logger.info(f"‚úÖ LLM Model: {llm_model}")\n        logger.info("‚úÖ All components initialized")\n    \n    def run(\n        self,\n        transcript_file: str,\n        output_csv: Optional[str] = None,\n        output_metrics: Optional[str] = None,\n        max_transcripts: Optional[int] = None,\n        domain: Optional[str] = None\n    ) -> List[str]:\n        """\n        Execute full CQG pipeline.\n        \n        Args:\n            transcript_file: Path to input JSON/CSV\n            output_csv: Path for output CSV (default from config)\n            output_metrics: Path for metrics JSON (default from config)\n            max_transcripts: Limit number of transcripts to process (not used in clustering load yet, but kept for API compat)\n            domain: Filter transcripts by domain\n        \n        Returns:\n            List of generated queries\n        """\n        self.stats["start_time"] = datetime.now()\n        \n        logger.info("\n" + "=" * 80)\n        logger.info("STARTING CAUSAL QUERY GENERATION PIPELINE")\n        logger.info("=" * 80)\n        \n        # Set defaults\n        output_csv = output_csv or str(Config.DEFAULT_OUTPUT_CSV)\n        output_metrics = output_metrics or str(Config.DEFAULT_METRICS_JSON)\n        \n        # Step 1: Action-Driven Semantic Clustering\n        # Note: extractor.analyze_transcripts handles loading internally for now\n        logger.info(f"üîÑ Phase 1: Analyzing transcripts from {transcript_file}...")\n        self.cluster_chars = self.extractor.analyze_transcripts(transcript_file, domain=domain)\n        \n        self.stats["clusters_found"] = len(self.cluster_chars)\n        logger.info(f"‚úÖ Found {self.stats['clusters_found']} action clusters")\n        \n        if not self.cluster_chars:\n            logger.error("No clusters found. Exiting.")\n            return []\n            \n        # Step 2: Query Generation\n        logger.info(f"üîÑ Phase 2: Generating queries...")\n        self.queries = self.generator.generate_queries(self.cluster_chars)\n        \n        self.stats["queries_generated"] = len(self.queries)\n        logger.info(f"‚úÖ Generated {self.stats['queries_generated']} queries")\n        \n        # Step 3: Export results\n        self._export_results(output_csv, output_metrics)\n        \n        # Finalize\n        self.stats["end_time"] = datetime.now()\n        self.stats["duration_seconds"] = (\n            self.stats["end_time"] - self.stats["start_time"]\n        ).total_seconds()\n        \n        self._print_summary()\n        \n        return self.queries\n\n    def run_task2_followupqg_llm_knowledge(\n        self,\n        qa_input_path: str,\n        output_followups_csv: Optional[str] = None\n    ) -> Dict:\n        """\n        Execute Task 2: Follow-upQG with LLM Internal Knowledge.\n        \n        REVISED: Extracts topics FROM QA PAIRS (not clusters)\n        \n        Generates follow-up questions using:\n        - Topics extracted from QA pairs (more reliable)\n        - LLM's internal knowledge\n        - 4-stage pipeline: Recognition ‚Üí Selection ‚Üí Fusion ‚Üí Generation\n        \n        Args:\n            qa_input_path: Path to JSON file with QA pairs from other team\n            output_followups_csv: Path for output CSV\n        \n        Returns:\n            Dictionary of enriched clusters with follow-ups\n        """\n        \n        logger.info("\n" + "=" * 80)\n        logger.info("TASK 2: Follow-upQG with LLM Internal Knowledge (QA-Driven)")\n        logger.info("=" * 80)\n        \n        if not self.cluster_chars:\n            logger.error("‚ùå No clusters from Task 1. Run Task 1 first.")\n            return {}\n        \n        logger.info(f"Processing {len(self.cluster_chars)} clusters from Task 1...")\n        \n        # Step 1: Load QA pairs from other team\n        logger.info("\nStep 1: Loading QA pairs from other team...")\n        qa_pairs = load_qa_pairs(qa_input_path)\n        if not qa_pairs:\n            logger.warning("‚ö†Ô∏è No QA pairs loaded")\n            return {}\n        logger.info(f"‚úÖ Loaded {len(qa_pairs)} QA pairs")\n        \n        # Step 2: Link QAs to clusters\n        logger.info("\nStep 2: Linking QA pairs to clusters...")\n        cluster_qa_map = link_qas_to_clusters(self.cluster_chars, qa_pairs)\n        linked_count = sum(1 for qas in cluster_qa_map.values() if qas)\n        logger.info(f"‚úÖ Linked QAs to {linked_count} clusters")\n        \n        # Initialize modules\n        logger.info("\nInitializing Task 2 modules...")\n        recognizer = RecognitionModule()\n        selector = SelectionModuleLLM(self.llm_client)\n        fusioner = FusionModuleLLM(self.llm_client)\n        generator = GenerationModuleLLM(self.llm_client)\n        logger.info("‚úÖ Modules initialized")\n        \n        enriched_clusters = {}\n        followups_generated = 0\n        \n        # Process each cluster\n        logger.info("\nStep 3-6: Processing clusters (Recognition ‚Üí Selection ‚Üí Fusion ‚Üí Generation)...")\n        for cid, cluster_data in tqdm(self.cluster_chars.items(), desc="Processing clusters"):\n            linked_qas = cluster_qa_map.get(cid, [])\n            action_label = cluster_data.get('action_label', 'unknown')\n            \n            if linked_qas:\n                try:\n                    # 1. Recognition: Extract topics FROM QA PAIRS (REVISED!)\n                    topics = recognizer.extract_topics_from_qas(\n                        linked_qas,\n                        cluster_action=action_label\n                    )\n                    \n                    # 2. Selection: LLM selects relevant concepts\n                    concepts = selector.select_relevant_concepts(\n                        action_label, topics, linked_qas\n                    )\n                    \n                    # 3. Fusion: LLM fuses knowledge with data\n                    fused_understanding = fusioner.fuse_knowledge_and_data(\n                        action_label, topics, concepts, linked_qas\n                    )\n                    \n                    # 4. Generation: Generate follow-ups\n                    followups = generator.generate_followups(\n                        action_label, topics, concepts, fused_understanding, linked_qas\n                    )\n                    \n                    cluster_data['deep_followups'] = followups\n                    cluster_data['topics'] = topics\n                    cluster_data['concepts'] = concepts\n                    cluster_data['fused_context'] = fused_understanding[:200]\n                    followups_generated += len(followups)\n                    \n                    logger.info(f"  ‚úÖ Cluster {cid} ({action_label}): {len(followups)} follow-ups")\n                \n                except Exception as e:\n                    logger.error(f"  ‚ùå Error processing cluster {cid}: {e}")\n                    cluster_data['deep_followups'] = []\n            \n            else:\n                logger.debug(f"  ‚ö†Ô∏è Cluster {cid} ({action_label}): No linked QAs")\n                cluster_data['deep_followups'] = []\n            \n            enriched_clusters[cid] = cluster_data\n        \n        logger.info(f"\n‚úÖ Generated {followups_generated} follow-up questions total")\n        \n        # Step 7: Export results\n        logger.info("\nStep 7: Exporting results...")\n        output_csv = output_followups_csv or str(Config.OUTPUT_DIR / "task2_followups.csv")\n        save_followups_csv(enriched_clusters, output_csv)\n        \n        logger.info("=" * 80)\n        logger.info(f"‚úÖ Task 2 Complete!")\n        logger.info(f"   Results saved to: {output_csv}")\n        logger.info(f"   Ready for other team to generate answers")\n        logger.info("=" * 80)\n        \n        return enriched_clusters\n    \n    def _export_results(self, output_csv: str, output_metrics: str) -> None:\n        """Export queries to CSV and metrics to JSON."""\n        logger.info("\n" + "=" * 80)\n        logger.info("EXPORTING RESULTS")\n        logger.info("=" * 80)\n        \n        # Ensure output directory exists\n        Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n        \n        if self.queries:\n            # Export CSV\n            df = pd.DataFrame(self.queries, columns=["Query"])\n            df.to_csv(output_csv, index=False)\n            logger.info(f"‚úÖ Exported {len(self.queries)} queries to: {output_csv}")\n            \n            # Export metrics\n            metrics = self.stats.copy()\n            # Add cluster summaries to metrics\n            metrics["clusters"] = {\n                cid: {\n                    "label": data["action_label"],\n                    "size": data["size"],\n                    "summary": data["summary"]\n                }\n                for cid, data in self.cluster_chars.items()\n            }\n            \n            with open(output_metrics, 'w') as f:\n                json.dump(metrics, f, indent=2, default=str)\n            logger.info(f"‚úÖ Exported metrics to: {output_metrics}")\n        else:\n            logger.warning("‚ö†Ô∏è  No queries generated. No CSV exported.")\n    \n    def _print_summary(self) -> None:\n        """Print execution summary."""\n        logger.info("\n" + "=" * 80)\n        logger.info("PIPELINE EXECUTION SUMMARY")\n        logger.info("=" * 80)\n        \n        logger.info(f"\nüìä Clusters Found: {self.stats['clusters_found']}")\n        logger.info(f"üìà Queries Generated: {self.stats['queries_generated']}")\n        logger.info(f"‚è±Ô∏è  Execution Time: {self.stats['duration_seconds']:.1f}s")\n        logger.info("\n" + "=" * 80)\n\nif __name__ == "__main__":\n    import argparse\n    \n    # Configure logging\n    logging.basicConfig(\n        level=getattr(logging, Config.LOG_LEVEL),\n        format=Config.LOG_FORMAT\n    )\n    \n    parser = argparse.ArgumentParser(description="Causal Query Generation Pipeline")\n    parser.add_argument("--input", type=str, default=str(Config.DEFAULT_INPUT_CSV), help="Path to input transcript file")\n    parser.add_argument("--output-csv", type=str, default=str(Config.DEFAULT_OUTPUT_CSV), help="Path to output CSV")\n    parser.add_argument("--output-metrics", type=str, default=str(Config.DEFAULT_METRICS_JSON), help="Path to output metrics JSON")\n    parser.add_argument("--domain", type=str, default=None, help="Filter transcripts by domain")\n    \n    args = parser.parse_args()\n    \n    pipeline = CQGPipeline()\n    pipeline.run(\n        transcript_file=args.input,\n        output_csv=args.output_csv,\n        output_metrics=args.output_metrics,\n        domain=args.domain\n    )\n